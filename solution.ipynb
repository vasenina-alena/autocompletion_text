{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07818ed3",
   "metadata": {},
   "source": [
    "# ПРОЕКТ: Нейросеть для автодополнения текстов\n",
    "* Реализована и обучена модель на основе рекуррентных нейронных сетей\n",
    "* Взята более «тяжёлая» предобученную модель из Transformers\n",
    "* Проведена оценка эфктивности двух моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a106c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data_utils import *\n",
    "from src.next_token_dataset import NextTokenDataset\n",
    "from src.lstm_model import LSTMTextGenerator\n",
    "from src.lstm_train import train_lstm_model \n",
    "from src.lstm_eval import evaluate_lstm\n",
    "from src.transformer_eval_pipline import evaluate_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987d91f",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5326ad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно переформатированы в data/raw_dataset.csv\n",
      "Очищенный датасет сохранен: data/dataset_processed.csv\n",
      "\n",
      "Датасет размерностью 1596876 строк разделен на:\n",
      "train: 1277500 \n",
      "val: 159688 \n",
      "test: 159688\n"
     ]
    }
   ],
   "source": [
    "# Подготовка данных\n",
    "save_tweets_to_csv()\n",
    "process_dataset()\n",
    "split_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe96886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(\"models/tokenizer/\")\n",
    "\n",
    "train_texts = pd.read_csv(\"data/train.csv\")['tweet'].tolist()[:638750]\n",
    "val_texts = pd.read_csv(\"data/val.csv\")['tweet'].tolist()[:79844]\n",
    "test_texts = pd.read_csv(\"data/test.csv\")['tweet'].tolist()[:79844]\n",
    "\n",
    "train_dataset = NextTokenDataset(train_texts, tokenizer, max_length=20)\n",
    "val_dataset = NextTokenDataset(val_texts, tokenizer, max_length=20)\n",
    "test_dataset = NextTokenDataset(test_texts, tokenizer, max_length=20)\n",
    "\n",
    "def collate_fn(batch, pad_token_id=50256):\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
    "    labels = [torch.tensor(item['labels']) for item in batch]\n",
    "\n",
    "    # Паддинг до максимальной длины в батче\n",
    "    input_ids = nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=pad_token_id)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return input_ids, labels\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d23e0c",
   "metadata": {},
   "source": [
    "# Реализация рекуррентной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8119f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2486/2486 [32:59<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.6611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2486/2486 [44:54<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 5.9016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2486/2486 [33:11<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 5.6904\n",
      "Модель сохранена: models/lstm_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Обучение LSTM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = train_lstm_model(train_loader, val_loader, vocab_size=tokenizer.vocab_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204c0d56",
   "metadata": {},
   "source": [
    "# Тренировка рекуррентной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe00fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasal\\AppData\\Local\\Temp\\ipykernel_1424\\802171707.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/lstm_model.pth', map_location=device))\n",
      "Evaluating LSTM: 100%|██████████| 311/311 [1:54:04<00:00, 22.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM ROUGE-1: 0.3534\n",
      "LSTM ROUGE-2: 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Устройство (CPU или GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Инициализация модели\n",
    "model = LSTMTextGenerator(vocab_size=tokenizer.vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=device))\n",
    "model.to(device)  # Переместите модель на нужное устройство\n",
    "model.eval()  # Переключите модель в режим оценки\n",
    "\n",
    "lstm_rouge1, lstm_rouge2 = evaluate_lstm(model, val_loader, tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308e9c1",
   "metadata": {},
   "source": [
    "Значения LSTM ROUGE-1 = 0.3534 и LSTM ROUGE-2 = 0.3050 указывает на то, что качество сгенерированного текста относительно низкое. \n",
    "<br>Модель может не передавать ключевые идеи или факты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62498c33",
   "metadata": {},
   "source": [
    "# Использование предобученного трансформенра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467c70f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilGPT-2: 100%|██████████| 1000/1000 [02:20<00:00,  7.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilGPT-2 (на 1000 примерах):\n",
      "  ROUGE-1: 0.6672\n",
      "  ROUGE-2: 0.6164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Устройство (CPU или GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_rouge1, transformer_rouge2 = evaluate_transformer(val_loader, tokenizer, device=device, max_examples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e8ef2",
   "metadata": {},
   "source": [
    "Значения ROUGE-1 и ROUGE-2 указывают на то, что модель генерирует текст, который достаточно близок к эталонному. \n",
    "<br>Также модель хорошо справляется с задачей извлечения ключевой информации и соблюдения структуры предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca193606",
   "metadata": {},
   "source": [
    "# Сравнение работы двух моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2876ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasal\\AppData\\Local\\Temp\\ipykernel_6704\\989992326.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/lstm_model.pth', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка автодополнения LSTM:\n",
      "Промпт: My friend and I play\n",
      "Дополнение LSTM: my friend and i play the new moon trailer i m so tired i m so tired i m so\n",
      "Промпт: I know who\n",
      "Дополнение LSTM: i know who i m so tired i m so tired i m so tired i m so tired i\n",
      "Промпт: I want to\n",
      "Дополнение LSTM: i want to go to the gym and i m so tired i m so tired i m so tired\n",
      "Промпт: We have a flat in\n",
      "Дополнение LSTM: we have a flat in the day i m so tired i m so tired i m so tired i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Промпт: I cook dinner\n",
      "Дополнение LSTM: i cook dinner and i m so tired i m so tired i m so tired i m so tired\n",
      "\n",
      "Оценка автодополнения DistilGPT:\n",
      "Промпт: My friend and I play\n",
      "Дополнение DistilGPT: My friend and I play online as they go on and on, but the other two never really played\n",
      "Промпт: I know who\n",
      "Дополнение DistilGPT: I know who I am — and have to be. Let's put this place in perspective. I\n",
      "Промпт: I want to\n",
      "Дополнение DistilGPT: I want to share this post (no one can be sure about where this post began). If you\n",
      "Промпт: We have a flat in\n",
      "Дополнение DistilGPT: We have a flat in West Palm Beach. The flat is 6.5 feet long and covers about\n",
      "Промпт: I cook dinner\n",
      "Дополнение DistilGPT: I cook dinner with you soon but then, you'll not be ready to cook the dish...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Сравнение результатов LSTM и DistilGPT-2\n",
    "# Загружаем модель DistilGPT-2\n",
    "\n",
    "# Инициализация модели\n",
    "model = LSTMTextGenerator(vocab_size=tokenizer.vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('models/lstm_model.pth', map_location=device))\n",
    "model.to(device)  # Переместите модель на нужное устройство\n",
    "\n",
    "generator_DistilGPT = pipeline(\"text-generation\", model=\"distilgpt2\", tokenizer=tokenizer)\n",
    "\n",
    "# Примеры промптов — начала фраз\n",
    "examples = [\n",
    "    \"My friend and I play\",\n",
    "    \"I know who\",\n",
    "    \"I want to\",\n",
    "    \"We have a flat in\",\n",
    "    \"I cook dinner\"\n",
    "]\n",
    "print(\"Оценка автодополнения LSTM:\")\n",
    "for prompt in examples:\n",
    "    generated = model.generate(tokenizer, prompt, max_length=20, device=device)\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение LSTM: {generated}\")\n",
    "print(\"\\nОценка автодополнения DistilGPT:\")\n",
    "for prompt in examples:\n",
    "    result = generator_DistilGPT(prompt, max_length=20, do_sample=True, top_k=50)\n",
    "    generated = result[0]['generated_text']\n",
    "    print(f\"Промпт: {prompt}\")\n",
    "    print(f\"Дополнение DistilGPT: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee23a8b",
   "metadata": {},
   "source": [
    "* **LSTM** модель демонстрирует проблемы с логикой и связностью текста, часто повторяя одни и те же фразы, что снижает качество автодополнения.\n",
    "* **DistilGPT** модель генерирует более содержательные и логичные продолжения, которые развивают идеи и создают интересный контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d21e9",
   "metadata": {},
   "source": [
    "# Оценка работы предобученного трансформера на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af1a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilGPT-2: 100%|██████████| 1000/1000 [01:48<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilGPT-2 (на 1000 примерах):\n",
      "  ROUGE-1: 0.6723\n",
      "  ROUGE-2: 0.6203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Устройство (CPU или GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer_rouge1, transformer_rouge2 = evaluate_transformer(test_loader, tokenizer, device=device, max_examples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b79615",
   "metadata": {},
   "source": [
    "# ВЫВОДЫ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff52969",
   "metadata": {},
   "source": [
    "В целом, **DistilGPT** показывает **значительно лучшие результаты** в плане качества и содержательности автодополнений по сравнению с LSTM.\n",
    "<br> на это указывают как значения rouge-1 и rouge-2, которые в два раза превосходят значения для LSTM, так и непосредственное сравнение полученных текстов.\n",
    "| Модель | rouge-1 | rouge-2|\n",
    "|--------|---------|--------|\n",
    "| LSTM| 0.3534 |  0.3050 |\n",
    "| DistilGPT-2 | 0.6672 | 0.6164 |\n",
    "\n",
    "На тестовой выборке DistilGPT-2 показала также хороший результат:\n",
    "  * ROUGE-1: 0.6723;\n",
    "  * ROUGE-2: 0.6203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73451bfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02859bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.8.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
